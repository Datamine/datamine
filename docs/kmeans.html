<!DOCTYPE html>
<html>
	<head>
		<link href="http://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet" type="text/css">
		<link href='http://fonts.googleapis.com/css?family=Merriweather:400,300' rel='stylesheet' type='text/css'>
		<link href="../globalstyle.css" rel="stylesheet" type="text/css">
		
		<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.2.0/styles/solarized-light.min.css">
		<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.2.0/highlight.min.js"></script>
    	<script>hljs.initHighlightingOnLoad();</script>


		<style type="text/css">
		<!--
			  .blockfont{ 
					font-variant: small-caps;
					text-decoration:none;
					color:#000000; }
		-->
		</style>

		<script type="text/x-mathjax-config">
		  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
		</script>
		<script type="text/javascript" async
		  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML">
		</script>

	
		<title>
			John Loeber: K-Means Clustering
		</title>
	</head>

	<body>
		<div style="width:751px;height:20px;background-color:#FFFFFF"></div>
			<div class="siteimg"></div>
			<div class="siteheader"> John Loeber </div>
		<div style="height:10px;width:490px"></div>
		
		<div style="width:167px;height:120px;float:left">
			<a href="../index.html"><div class="block" id="linkblock"><span class="blockfont">Projects</span></div></a>
			<div style="width:10px;height:120px;float:right"></div>
		</div>

		<div style="width:167px;height:120px;float:left">
			<a href="../writing.html"><div class="block" id="linkblock"><span class="blockfont">Writing</span></div></a>
			<div style="width:10px;height:120px;float:right"></div>
		</div>

		<a href="../about.html"><div class="block" id="linkblock"><span class="blockfont">About</span></div></a>
			
		<div class="main">
			<br>
			<atitle>K-Means Clustering on Handwritten Digits</atitle></td>
			<p>
			K-Means Clustering is a machine learning technique for classifying data. It&rsquo;s best explained with a simple example. Below is some (fictitious) data comparing elephants and penguins. We've plotted 20 animals, and each one is represented by a (weight, height) coordinate.<br>
			<img class="imgcenter" src="../images/animals_kmeans.png"></img>
			<p>
			You can see that the coordinate points of the elephants and penguins form two clusters: elephants are bigger and heavier, penguins are smaller and lighter. Now suppose we've got one more datapoint, but we've forgotten whether it's an elephant or a penguin. Let's plot it, too. We've marked it in orange.
			</p>
			<img class="imgcenter" src="../images/animals_kmeans1.png"></img>
			<p>
			If you were to make a guess, you'd say that the orange datapoint probably belongs to an elephant, and not to a penguin. We say this because the orange datapoint seems to belong to the elephant cluster, not to the penguin cluster.
			</p>
			<p>
			This is the essence of k-means clustering. We take some labelled data &mdash; like heights and weights of animals. In our example, &ldquo;penguin&rdquo; or &ldquo;elephant&rdquo; is a label. We use an algorithm to figure out which points belong to which clusters. We look at the labels of the clusters to understand what label each cluster corresponds to. Then we take an unlabelled datapoint, see into which cluster it fits best, and thereby assign the unlabelled datapoint a label. 
			</p>
			<p>
			We call the process k-means clustering because we assume that there are $k$ clusters, and each cluster is defined by its center point &mdash; its mean. To find these clusters, we use <i>Lloyd's Algorithm</i>: we start out with $k$ random centroids. A centroid is simply a datapoint around which we form a cluster. For each centroid, we find the datapoints that are closer to that centroid than to any other centroid. We call that set of datapoints its cluster. Then we take the mean of the cluster, and let that be the new centroid. We say the algorithm has converged<small><sup><a href="#footnote0" name="note0">[0]</a></sup></small> when it stops moving the centroids.
			</p>
			<p>
			We'll use k-means clustering on the <a href="https://en.wikipedia.org/wiki/MNIST_database">MNIST dataset of handwritten digits</a>, which consists of 60,000 handwritten digits (0-9) that have been scanned in and scaled to 28 $\times$ 28 pixels. They look like this:
			</p>
			<img class="imgcenter" src="../images/mnist.png"></img>
			<p>			
			For every digit, each pixel can be represented as an integer in the range [0,255] where 0 corresponds to the pixel being completely white, and 255 corresponds to the pixel being completely black. This gives us a 28 $\times$ 28 matrix of integers. We can then flatten this matrix into a 784 $\times$ 1 vector, which is like a coordinate pair, except for that instead of 2 coordinates it has 784. Now that the data is in coordinate form, we can run k-means clustering. Let&rsquo;s do it.
			</p>
			<p>
			We start by importing all required libraries.
			</p>
<pre><code class="python">import random
from base64 import b64decode
from json import loads
import numpy as np
import matplotlib.pyplot as plt
# set matplotlib to display all plots inline with the notebook
%matplotlib inline</code></pre>
			<p>
			Next, we write a function to read in the MNIST data.
			</p>
<pre><code class="python">def parse(x):
    """
    to parse the digits file into tuples of 
    (labelled digit, numpy array of vector representation of digit)
    """
    digit = loads(x)
    array = np.fromstring(b64decode(digit["data"]),dtype=np.ubyte)
    array = array.astype(np.float64)
    return (digit["label"], array)</code></pre>
    		<p>
    		Then we use that function to read in the data. We read every datapoint into a tuple containing a label and the data vector. 
    		</p>
<pre><code> class="python"># read in the digits file. Digits is a list of 60,000 tuples,
# each containing a labelled digit and its vector representation.
with open("digits.base64.json","r") as f:
    digits = map(parse, f.readlines())</code></pre>
    		<p>
    		Then we split the data into a <i>training</i> and a <i>validation</i> set. We'll construct our clusters with the training set, and then use those clusters to classify the datapoints in the validation set. We can then check those classifications against the labels to see how often the algorithm misclassifies a datapoint.
    		</p>
<pre><code># pick a ratio for splitting the digits list into a training and a validation set.
ratio = int(len(digits)*0.25)
validation = digits[:ratio]
training = digits[ratio:]</code></pre>
			<p>
			Now we write a function to take a datapoint and display the digit. This is mostly for debugging and checking our results.
			</p>
<pre><code>def display_digit(digit, labeled = True, title = ""):
    """ 
    graphically displays a 784x1 vector, representing a digit
    """
    if labeled:
        digit = digit[1]
    image = digit
    plt.figure()
    fig = plt.imshow(image.reshape(28,28))
    fig.set_cmap('gray_r')
    fig.axes.get_xaxis().set_visible(False)
    fig.axes.get_yaxis().set_visible(False)
    if title != "":
        plt.title("Inferred label: " + str(title))</code></pre>
        	<p>
        	Now we begin writing Lloyd's algorithm. There are many libraries that have already implemented this algorithm, but it's good practice to write by hand. Notice that the <i>means</i> in k-means clustering comes from the function in which we take the mean of a cluster, and relocate the centroid to that mean. A mean, however, is not robust to outliers. It's possible to take the median instead of the mean &mdash; that's known as <a href="https://en.wikipedia.org/wiki/K-medians_clustering">k-medians clustering</a>. As usual, there are many variants of this algorithm for various use cases. Following are helper functions for Lloyd's algorithm.
        	</p>
<pre><code>def init_centroids(labelled_data,k):
    """
    randomly pick some k centers from the data as starting values for centroids.
    Remove labels.
    """
    return map(lambda x: x[1], random.sample(labelled_data,k))

def sum_cluster(labelled_cluster):
    """
    from http://stackoverflow.com/questions/20640396/quickly-summing-numpy-arrays-element-wise
    element-wise sums a list of arrays. assumes all datapoints in labelled_cluster are labelled.
    """
    # assumes len(cluster) > 0
    sum_ = labelled_cluster[0][1].copy()
    for (label,vector) in labelled_cluster[1:]:
        sum_ += vector
    return sum_

def mean_cluster(labelled_cluster):
    """
    computes the mean (i.e. the centroid at the middle) of a list of vectors (a cluster).
    take the sum and then divide by the size of the cluster.
    assumes all datapoints in labelled_cluster are labelled.
    """
    sum_of_points = sum_cluster(labelled_cluster)
    mean_of_points = sum_of_points * (1.0 / len(labelled_cluster))
    return mean_of_points</code></pre>
    		<p>
    		Finally, the main parts of Lloyd's algorithm: forming clusters and moving centroids.
    		</p>
<pre><code>def form_clusters(labelled_data, unlabelled_centroids):
    """
    given some data and centroids for the data, allocate each datapoint
    to its closest centroid. This forms clusters.
    """
    # enumerate because centroids are arrays which are unhashable,
    centroids_indices = range(len(unlabelled_centroids))
    
    # initialize an empty list for each centroid. The list will contain
    # all the datapoints that are closer to that centroid than to any other.
    # That list is the cluster of that centroid.
    clusters = {c: [] for c in centroids_indices}
    
    for (label,Xi) in labelled_data:
        # for each datapoint, pick the closest centroid.
        smallest_distance = float("inf")
        for cj_index in centroids_indices:
            cj = unlabelled_centroids[cj_index]
            distance = np.linalg.norm(Xi - cj)
            if distance < smallest_distance:
                closest_centroid_index = cj_index
                smallest_distance = distance
        # allocate that datapoint to the cluster of that centroid.
        clusters[closest_centroid_index].append((label,Xi))
    return clusters.values()

def move_centroids(labelled_clusters):
    """
    returns a list of centroids corresponding to the clusters.
    """
    new_centroids = []
    for cluster in labelled_clusters:
        new_centroids.append(mean_cluster(cluster))
    return new_centroids

def repeat_until_convergence(labelled_data, labelled_clusters, unlabelled_centroids):
    """
    form clusters around centroids, then keep moving the centroids
    until the moves are no longer significant, i.e. we've found
    the best-fitting centroids for the data.
    """
    previous_max_difference = 0
    while True:
        unlabelled_old_centroids = unlabelled_centroids
        unlabelled_centroids = move_centroids(labelled_clusters)
        labelled_clusters = form_clusters(labelled_data, unlabelled_centroids)
        # we keep old_clusters and clusters so we can get the maximum difference
        # between centroid positions every time. we say the centroids have converged
        # when the maximum difference between centroid positions is small.   
        differences = map(lambda a, b: np.linalg.norm(a-b),unlabelled_old_centroids,unlabelled_centroids)
        max_difference = max(differences)
        difference_change = abs((max_difference-previous_max_difference)/np.mean([previous_max_difference,max_difference])) * 100
        previous_max_difference = max_difference
        # difference change is nan once the list of differences is all zeroes.
        if np.isnan(difference_change):
            break
    return labelled_clusters, unlabelled_centroids</code></pre>




			FORMATTING: APOSTROPHES ETC

			<br>
			<sh>Acknowledgements</sh>
			<p>
				This project was inspired by a homework assignment in John Lafferty's <a href="https://galton.uchicago.edu/~lafferty/37601-syllabus.pdf">Large-Scale Data Analysis</a> course that I took at UChicago in the Spring of 2015. I collaborated with Elliott Ding on that assignment. In the class, we used Apache Spark and a map-reduce framework on AWS to take advantage of parallelization. To make the algorithm more accessible, I've rewritten the code for this article to not use distributed systems.
			</p>

			<sh>Download</sh>
			<p>
				A GitHub repository containing the iPython notebook, dataset, etc. is available <a href="https://github.com/Datamine/MNIST-K-Means-Clustering">here</a>.
			</p>
			
			<sh>Notes</sh>
			<p>
			<a href="#note0" name="footnote0">[0]</a> Lloyd's algorithm converges only to a local optimum. Lloyd's algorithm does not guarantee finding a global optimum. This can be a critical pitfall.
			</p>
			<div class="notice">
				<br><br>This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.
			</div>
		</div>
	</body>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-74324013-1', 'auto');
  ga('send', 'pageview');

</script>
</html>