<!DOCTYPE html>
<html>
	<head>
		<link href="http://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet" type="text/css">
		<link href='http://fonts.googleapis.com/css?family=Merriweather:400,300' rel='stylesheet' type='text/css'>
		<link href="../globalstyle.css" rel="stylesheet" type="text/css">
		
		<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.2.0/styles/solarized-light.min.css">
		<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.2.0/highlight.min.js"></script>
    	<script>hljs.initHighlightingOnLoad();</script>


		<style type="text/css">
		<!--
			  .blockfont{ 
					font-variant: small-caps;
					text-decoration:none;
					color:#000000; }
		-->
		</style>

		<script type="text/x-mathjax-config">
		  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
		</script>
		<script type="text/javascript" async
		  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML">
		</script>

	
		<title>
			John Loeber: K-Means Clustering
		</title>
	</head>

	<body>
		<div style="width:751px;height:20px;background-color:#FFFFFF"></div>
			<div class="siteimg"></div>
			<div class="siteheader"> John Loeber </div>
		<div style="height:10px;width:490px"></div>
		
		<div style="width:167px;height:120px;float:left">
			<a href="../index.html"><div class="block" id="linkblock"><span class="blockfont">Projects</span></div></a>
			<div style="width:10px;height:120px;float:right"></div>
		</div>

		<div style="width:167px;height:120px;float:left">
			<a href="../writing.html"><div class="block" id="linkblock"><span class="blockfont">Writing</span></div></a>
			<div style="width:10px;height:120px;float:right"></div>
		</div>

		<a href="../about.html"><div class="block" id="linkblock"><span class="blockfont">About</span></div></a>
			
		<div class="main">
			<br>
			<atitle>K-Means Clustering on Handwritten Digits</atitle></td>
			<p>
			K-Means Clustering is a machine learning technique for classifying data. It&rsquo;s best explained with a simple example. Below is some (fictitious) data comparing elephants and penguins. We've plotted 20 animals, and each one is represented by a (weight, height) coordinate.<br>
			<img class="imgcenter" src="../images/animals_kmeans.png"></img>
			<p>
			You can see that the coordinate points of the elephants and penguins form two clusters: elephants are bigger and heavier, penguins are smaller and lighter. Now suppose we've got one more datapoint, but we've forgotten whether it's an elephant or a penguin. Let's plot it, too. We've marked it in orange.
			</p>
			<img class="imgcenter" src="../images/animals_kmeans1.png"></img>
			<p>
			If you were to make a guess, you'd say that the orange datapoint probably belongs to an elephant, and not to a penguin. We say this because the orange datapoint seems to belong to the elephant cluster, not to the penguin cluster.
			</p>
			<p>
			This is the essence of K-Means clustering. We take some labelled data &mdash; like heights and weights of animals. In our example, &ldquo;penguin&rdquo; or &ldquo;elephant&rdquo; is a label. We use an algorithm to figure out which points belong to which clusters. We look at the labels of the clusters to understand what label each cluster corresponds to. Then we take an unlabelled datapoint, see into which cluster it fits best, and thereby assign the unlabelled datapoint a label. 
			</p>
			<p>
			We call the process K-Means clustering because we assume that there are $k$ clusters, and each cluster is defined by its center point &mdash; its mean. To find these clusters, we use <i>Lloyd's Algorithm</i>: we start out with $k$ random centroids. A centroid is simply a datapoint around which we form a cluster. For each centroid, we find the datapoints that are closer to that centroid than to any other centroid. We call that set of datapoints its cluster. Then we take the mean of the cluster, and let that be the new centroid. We say the algorithm has converged<small><sup><a href="#footnote0" name="note0">[0]</a></sup></small> when it stops moving the centroids.
			</p>
			<p>
			We'll use K-Means clustering on the <a href="https://en.wikipedia.org/wiki/MNIST_database">MNIST dataset of handwritten digits</a>, which consists of 60,000 handwritten digits (0-9) that have been scanned in and scaled to 28 $\times$ 28 pixels. They look like this:
			</p>
			<img class="imgcenter" src="../images/mnist.png"></img>
			<p>			
			For every digit, each pixel can be represented as an integer in the range [0,255] where 0 corresponds to the pixel being completely white, and 255 corresponds to the pixel being completely black. This gives us a 28 $\times$ 28 matrix of integers. We can then flatten this matrix into a 784 $\times$ 1 vector, which is like a coordinate pair, except for that instead of 2 coordinates it has 784. Now that the data is in coordinate form, we can cluster it.
			</p>
			<p>
			We start by importing all required libraries.
			</p>
<pre><code class="python">import random
from base64 import b64decode
from json import loads
import numpy as np
import matplotlib.pyplot as plt
# set matplotlib to display all plots inline with the notebook
%matplotlib inline</code></pre>



			<img width="751px" style="border:1px solid black" src="../images/Q-Talk/Slide48.PNG"></img>
			<p>
			Thanks for your time. Questions?
			</p>
			FORMATTING: APOSTROPHES ETC

			<br>
			<sh>Acknowledgements</sh>
			<p>
				This project was inspired by a homework assignment in John Lafferty's <a href="https://galton.uchicago.edu/~lafferty/37601-syllabus.pdf">Large-Scale Data Analysis</a> course that I took at UChicago in the Spring of 2015. I collaborated with Elliott Ding on that assignment. In the class, we used Apache Spark and a map-reduce framework on AWS to take advantage of parallelization. To make the algorithm more accessible, I've rewritten the code for this article to not use distributed systems.
			</p>

			<sh>Download</sh>
			<p>
				A GitHub repository containing the iPython notebook, dataset, etc. is available <a href="https://github.com/Datamine/MNIST-K-Means-Clustering">here</a>.
			</p>
			
			<sh>Notes</sh>
			<p>
			<a href="#note0" name="footnote0">[0]</a> Lloyd's algorithm converges only to a local optimum. Lloyd's algorithm does not guarantee finding a global optimum. This can be a critical pitfall.
			</p>
			<div class="notice">
				<br><br>This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.
			</div>
		</div>
	</body>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-74324013-1', 'auto');
  ga('send', 'pageview');

</script>
</html>